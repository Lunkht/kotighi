# ============================================================
#  SENTINEL AI â€” CybersÃ©curitÃ© avec Dataset NSL-KDD
#  Ce fichier utilise les VRAIES colonnes du dataset NSL-KDD
#  (le standard mondial pour la dÃ©tection d'intrusion rÃ©seau)
# ============================================================
#
#  INSTALLATION (une seule fois) :
#  pip install scikit-learn pandas numpy matplotlib seaborn
#
#  COMMENT OBTENIR LE VRAI DATASET NSL-KDD :
#  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Option A â€” Kaggle (recommandÃ©) :
#    1. Va sur https://www.kaggle.com
#    2. CrÃ©e un compte gratuit
#    3. Recherche : "NSL-KDD Dataset"
#    4. TÃ©lÃ©charge "KDDTrain+.txt" et "KDDTest+.txt"
#    5. Place ces fichiers dans le mÃªme dossier que ce script
#    6. Change USE_REAL_DATA = True (ligne 45)
#
#  Option B â€” Direct :
#    https://www.unb.ca/cic/datasets/nsl.html
# ============================================================

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (accuracy_score, classification_report,
                             confusion_matrix, roc_auc_score)
from sklearn.preprocessing import LabelEncoder, StandardScaler

# â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Mets True quand tu as tÃ©lÃ©chargÃ© les vrais fichiers Kaggle
USE_REAL_DATA = False
TRAIN_FILE    = "KDDTrain+.txt"   # chemin vers ton fichier tÃ©lÃ©chargÃ©
TEST_FILE     = "KDDTest+.txt"

# â”€â”€ COLONNES OFFICIELLES DU DATASET NSL-KDD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ce sont les 41 caractÃ©ristiques rÃ©seau rÃ©elles + 1 label
COLONNES_NSLKDD = [
    # CaractÃ©ristiques de base de la connexion TCP
    'duree', 'protocole', 'service', 'flag',
    'octets_src_dest', 'octets_dest_src',
    'connexion_land', 'fragments_errones', 'urgent',
    # CaractÃ©ristiques du contenu
    'connexions_root', 'connexions_su', 'acces_root',
    'creation_fichier', 'shells', 'acces_fichiers',
    'commandes_sortie', 'connexion_hot', 'echecs_login',
    'login_reussi', 'nb_compromis', 'root_shell',
    'su_tente', 'nb_root', 'nb_creation_fichier',
    'nb_shells', 'nb_acces_fichiers', 'nb_commandes_sortie',
    'connexion_hote_login', 'connexion_invite',
    # CaractÃ©ristiques du trafic (fenÃªtre 2 secondes)
    'nb_connexions', 'nb_srv_connexions',
    'taux_erreur_serv', 'taux_erreur_rerr',
    'taux_erreur_serr', 'taux_srv_diff_hote',
    'taux_srv_hote', 'nb_hote_dest',
    'nb_hote_srv', 'taux_connexion_hote',
    'taux_srv_connexion_hote', 'taux_diff_srv_hote',
    # Label + score de difficultÃ©
    'label', 'score_difficulte'
]

# Types d'attaques dans NSL-KDD (regroupÃ©es en 4 catÃ©gories)
CATEGORIES_ATTAQUES = {
    # DoS : Ã©puiser les ressources du serveur
    'neptune': 'DoS', 'back': 'DoS', 'land': 'DoS',
    'pod': 'DoS', 'smurf': 'DoS', 'teardrop': 'DoS',
    'apache2': 'DoS', 'udpstorm': 'DoS',
    # Probe : reconnaissance / scan
    'ipsweep': 'Probe', 'nmap': 'Probe', 'portsweep': 'Probe',
    'satan': 'Probe', 'mscan': 'Probe', 'saint': 'Probe',
    # R2L : accÃ¨s distant non autorisÃ©
    'ftp_write': 'R2L', 'guess_passwd': 'R2L', 'imap': 'R2L',
    'multihop': 'R2L', 'phf': 'R2L', 'spy': 'R2L',
    'warezclient': 'R2L', 'warezmaster': 'R2L',
    # U2R : escalade de privilÃ¨ges
    'buffer_overflow': 'U2R', 'loadmodule': 'U2R',
    'perl': 'U2R', 'rootkit': 'U2R', 'ps': 'U2R',
    # Normal
    'normal': 'Normal'
}


# ==============================================================
#  Ã‰TAPE 1 : CHARGEMENT DES DONNÃ‰ES
# ==============================================================

def charger_donnees():
    print("\n" + "="*60)
    print("  ğŸ“¦ Ã‰TAPE 1 : Chargement du dataset NSL-KDD")
    print("="*60)

    if USE_REAL_DATA:
        # â”€â”€ VRAI DATASET KAGGLE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(f"  ğŸ“‚ Lecture du fichier rÃ©el : {TRAIN_FILE}")
        try:
            df_train = pd.read_csv(TRAIN_FILE, header=None, names=COLONNES_NSLKDD)
            df_test  = pd.read_csv(TEST_FILE,  header=None, names=COLONNES_NSLKDD)
            df = pd.concat([df_train, df_test], ignore_index=True)
            print(f"  âœ… {len(df_train)} lignes train + {len(df_test)} lignes test")
        except FileNotFoundError:
            print("  âŒ Fichier non trouvÃ©. Passe USE_REAL_DATA = False")
            print("     ou place KDDTrain+.txt dans ce dossier.")
            return None

    else:
        # â”€â”€ DONNÃ‰ES SIMULÃ‰ES (fidÃ¨les au vrai NSL-KDD) â”€â”€â”€â”€
        print("  ğŸ”¬ GÃ©nÃ©ration de donnÃ©es simulÃ©es (structure NSL-KDD rÃ©elle)...")
        print("  ğŸ’¡ Pour utiliser le vrai dataset â†’ USE_REAL_DATA = True")
        np.random.seed(42)
        N = 5000

        # Simule les 41 colonnes avec des distributions rÃ©alistes
        df = pd.DataFrame({
            'duree':              np.random.exponential(5, N).astype(int),
            'protocole':          np.random.choice(['tcp','udp','icmp'], N, p=[0.6,0.3,0.1]),
            'service':            np.random.choice(['http','ftp','smtp','ssh','dns'], N),
            'flag':               np.random.choice(['SF','S0','REJ','RSTO'], N, p=[0.7,0.1,0.1,0.1]),
            'octets_src_dest':    np.random.exponential(5000, N).astype(int),
            'octets_dest_src':    np.random.exponential(3000, N).astype(int),
            'connexion_land':     np.random.choice([0,1], N, p=[0.99,0.01]),
            'fragments_errones':  np.zeros(N, dtype=int),
            'urgent':             np.zeros(N, dtype=int),
            'connexions_root':    np.random.choice([0,1,2], N, p=[0.8,0.15,0.05]),
            'echecs_login':       np.random.choice([0,1], N, p=[0.9,0.1]),
            'login_reussi':       np.random.choice([0,1], N, p=[0.85,0.15]),
            'nb_connexions':      np.random.randint(1, 512, N),
            'nb_srv_connexions':  np.random.randint(1, 512, N),
            'taux_erreur_serv':   np.random.uniform(0, 1, N).round(2),
            'taux_erreur_rerr':   np.random.uniform(0, 1, N).round(2),
            'nb_hote_dest':       np.random.randint(1, 255, N),
            'nb_hote_srv':        np.random.randint(1, 255, N),
            'taux_connexion_hote':np.random.uniform(0, 1, N).round(2),
        })

        # GÃ©nÃ¨re les labels (types d'attaques rÃ©els de NSL-KDD)
        types = ['normal', 'neptune', 'ipsweep', 'portsweep',
                 'satan', 'smurf', 'nmap', 'back',
                 'guess_passwd', 'buffer_overflow']
        probs = [0.52, 0.15, 0.08, 0.05, 0.05, 0.05, 0.04, 0.03, 0.02, 0.01]
        df['label'] = np.random.choice(types, N, p=probs)

    # Ajouter la catÃ©gorie (Normal / DoS / Probe / R2L / U2R)
    df['categorie'] = df['label'].map(
        lambda x: CATEGORIES_ATTAQUES.get(x, 'Autre')
    )

    print(f"\n  ğŸ“Š Distribution des connexions :")
    for cat, count in df['categorie'].value_counts().items():
        pct = count / len(df) * 100
        bar = 'â–ˆ' * int(pct / 2)
        print(f"     {cat:<10} {bar:<25} {count:>5} ({pct:.1f}%)")

    return df


# ==============================================================
#  Ã‰TAPE 2 : PRÃ‰TRAITEMENT
# ==============================================================

def pretraiter(df):
    print("\n" + "="*60)
    print("  ğŸ”§ Ã‰TAPE 2 : PrÃ©traitement des donnÃ©es")
    print("="*60)

    # SÃ©lectionner les colonnes numÃ©riques disponibles
    cols_numeriques = df.select_dtypes(include=[np.number]).columns.tolist()
    cols_numeriques = [c for c in cols_numeriques
                       if c not in ['label', 'score_difficulte', 'categorie']]

    # Encoder les colonnes texte (protocole, service, flag)
    cols_texte = ['protocole', 'service', 'flag']
    encodeurs = {}
    for col in cols_texte:
        if col in df.columns:
            enc = LabelEncoder()
            df[col + '_enc'] = enc.fit_transform(df[col].astype(str))
            encodeurs[col] = enc
            cols_numeriques.append(col + '_enc')

    # Features et target
    X = df[cols_numeriques].fillna(0)
    y_multi  = df['categorie']                  # classification multi-classes
    y_binaire = (df['categorie'] != 'Normal').astype(int)  # normal vs attaque

    # Normalisation
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled = pd.DataFrame(X_scaled, columns=cols_numeriques)

    print(f"  âœ… {X.shape[1]} features sÃ©lectionnÃ©es")
    print(f"  âœ… {len(X)} connexions prÃªtes")
    print(f"  âœ… DonnÃ©es normalisÃ©es (StandardScaler)")

    return X_scaled, y_binaire, y_multi, scaler, encodeurs


# ==============================================================
#  Ã‰TAPE 3 : ENTRAÃNEMENT ET COMPARAISON DE MODÃˆLES
# ==============================================================

def entrainer_modeles(X, y_bin, y_multi):
    print("\n" + "="*60)
    print("  ğŸ§  Ã‰TAPE 3 : EntraÃ®nement et comparaison des modÃ¨les")
    print("="*60)

    # Split train / test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_bin, test_size=0.2, random_state=42, stratify=y_bin
    )

    # â”€â”€ MODÃˆLE 1 : Random Forest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n  ğŸŒ³ ModÃ¨le 1 : Random Forest...")
    rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)
    rf.fit(X_train, y_train)
    pred_rf = rf.predict(X_test)
    acc_rf  = accuracy_score(y_test, pred_rf) * 100

    # â”€â”€ MODÃˆLE 2 : Gradient Boosting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("  âš¡ ModÃ¨le 2 : Gradient Boosting...")
    gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
    gb.fit(X_train, y_train)
    pred_gb = gb.predict(X_test)
    acc_gb  = accuracy_score(y_test, pred_gb) * 100

    print(f"\n  ğŸ“Š RÃ©sultats comparatifs :")
    print(f"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print(f"  â”‚ ModÃ¨le                  â”‚ PrÃ©cision â”‚")
    print(f"  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"  â”‚ ğŸŒ³ Random Forest        â”‚  {acc_rf:>6.2f}%  â”‚")
    print(f"  â”‚ âš¡ Gradient Boosting    â”‚  {acc_gb:>6.2f}%  â”‚")
    print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

    # Choisir le meilleur modÃ¨le
    meilleur = rf if acc_rf >= acc_gb else gb
    nom_meilleur = "Random Forest" if acc_rf >= acc_gb else "Gradient Boosting"
    print(f"\n  ğŸ† Meilleur modÃ¨le : {nom_meilleur}")

    # Rapport dÃ©taillÃ©
    pred_best = meilleur.predict(X_test)
    print(f"\n  ğŸ“‹ Rapport dÃ©taillÃ© ({nom_meilleur}) :")
    print(classification_report(
        y_test, pred_best,
        target_names=['Normal', 'Attaque'],
        zero_division=0
    ))

    return meilleur, X_test, y_test


# ==============================================================
#  Ã‰TAPE 4 : IMPORTANCE DES FEATURES
# ==============================================================

def analyser_features(modele, colonnes):
    print("\n" + "="*60)
    print("  ğŸ” Ã‰TAPE 4 : Quelles features sont les plus importantes ?")
    print("="*60)
    print("  (Ce sont les indicateurs que l'IA utilise le plus)\n")

    importances = pd.Series(modele.feature_importances_, index=colonnes)
    top10 = importances.nlargest(10)

    for feature, score in top10.items():
        bar = 'â–ˆ' * int(score * 200)
        print(f"  {feature:<30} {bar:<20} {score:.4f}")


# ==============================================================
#  Ã‰TAPE 5 : DÃ‰TECTEUR EN TEMPS RÃ‰EL (simulation)
# ==============================================================

def detecteur_temps_reel(modele, scaler, feature_cols):
    print("\n" + "="*60)
    print("  ğŸš¨ Ã‰TAPE 5 : Simulation de dÃ©tection en temps rÃ©el")
    print("="*60)

    # Simule 8 connexions arrivant en "temps rÃ©el"
    scenarios = [
        {"nom": "Navigation web normale",        "octets_src_dest": 1200,  "nb_connexions": 15,  "taux_erreur_serv": 0.01, "duree": 30},
        {"nom": "Scan de ports (Probe)",          "octets_src_dest": 0,     "nb_connexions": 511, "taux_erreur_serv": 0.90, "duree": 0 },
        {"nom": "Flood DDoS (DoS)",               "octets_src_dest": 0,     "nb_connexions": 500, "taux_erreur_serv": 0.95, "duree": 0 },
        {"nom": "Transfert FTP lÃ©gitime",         "octets_src_dest": 50000, "nb_connexions": 5,   "taux_erreur_serv": 0.00, "duree": 120},
        {"nom": "Brute force SSH (R2L)",          "octets_src_dest": 200,   "nb_connexions": 300, "taux_erreur_serv": 0.80, "duree": 1 },
        {"nom": "RequÃªte DNS normale",            "octets_src_dest": 100,   "nb_connexions": 3,   "taux_erreur_serv": 0.00, "duree": 1 },
        {"nom": "Exploit buffer overflow (U2R)", "octets_src_dest": 300,   "nb_connexions": 10,  "taux_erreur_serv": 0.50, "duree": 2 },
        {"nom": "Streaming vidÃ©o",                "octets_src_dest": 80000, "nb_connexions": 20,  "taux_erreur_serv": 0.02, "duree": 600},
    ]

    print(f"\n  {'Connexion':<38} {'Verdict':<15} {'Confiance'}")
    print(f"  {'-'*65}")

    for s in scenarios:
        # CrÃ©er un vecteur de features (0 pour celles non spÃ©cifiÃ©es)
        vec = pd.DataFrame([{col: 0 for col in feature_cols}])
        for key, val in s.items():
            if key in vec.columns:
                vec[key] = val

        vec_scaled = scaler.transform(vec)
        pred  = modele.predict(vec_scaled)[0]
        proba = modele.predict_proba(vec_scaled)[0]
        conf  = max(proba) * 100

        if pred == 0:
            verdict = "ğŸŸ¢ NORMAL"
        else:
            verdict = "ğŸ”´ ATTAQUE"

        print(f"  {s['nom']:<38} {verdict:<15} {conf:.0f}%")


# ==============================================================
#  PROGRAMME PRINCIPAL
# ==============================================================

if __name__ == "__main__":
    print("\n" + "ğŸ›¡ï¸ " * 20)
    print("  SENTINEL AI â€” CybersÃ©curitÃ© avec Dataset NSL-KDD")
    print("ğŸ›¡ï¸ " * 20)

    # 1. Charger les donnÃ©es
    df = charger_donnees()
    if df is None:
        exit()

    # 2. PrÃ©traiter
    X, y_bin, y_multi, scaler, encodeurs = pretraiter(df)

    # 3. EntraÃ®ner et comparer les modÃ¨les
    meilleur_modele, X_test, y_test = entrainer_modeles(X, y_bin, y_multi)

    # 4. Analyser l'importance des features
    analyser_features(meilleur_modele, X.columns)

    # 5. Simulation temps rÃ©el
    detecteur_temps_reel(meilleur_modele, scaler, X.columns)

    print("\n" + "="*60)
    print("  âœ… Analyse complÃ¨te terminÃ©e !")
    print("="*60)
    print("""
  ğŸ“š Prochaines Ã©tapes :
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â‘  TÃ©lÃ©charge le vrai dataset NSL-KDD sur Kaggle :
      â†’ kaggle.com â†’ search "NSL-KDD Dataset"
      â†’ tÃ©lÃ©charge KDDTrain+.txt et KDDTest+.txt
      â†’ mets USE_REAL_DATA = True dans ce fichier

  â‘¡ AmÃ©liore le modÃ¨le :
      â†’ Teste XGBoost  : pip install xgboost
      â†’ Teste un rÃ©seau de neurones avec Keras

  â‘¢ CrÃ©e une interface web :
      â†’ pip install streamlit
      â†’ on code ensemble l'app visuelle !
  """)
